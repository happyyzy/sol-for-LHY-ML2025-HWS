# Goal
æœ¬æ¬¡ä½œä¸šçš„ç›®æ ‡æ˜¯ï¼šè®­ç»ƒä¸€ä¸ªencoder-onlyçš„transformersæ¥åšä¸€ä¸ªè‡ªå›å½’çš„å®å¯æ¢¦ç”Ÿæˆå™¨,baselineså¦‚ä¸‹
### ğŸ§® Baseline Evaluation Summary

| **Category** | **FID** | **PDR Estimate** | **Training Time** | **Score** |
|:------------:|:-------:|:----------------:|:-----------------:|:---------:|
| Public Simple Baseline  | â‰¤ 84.50 | â‰¥ 0.1 | 10 mins | +1pt |
| Private Simple Baseline | â‰¤ 84.50 | â‰¥ 0.1 | â€” | +1pt |
| Public Medium Baseline  | â‰¤ 81.00 | â‰¥ 0.5 | 20 mins | +1pt |
| Private Medium Baseline | â‰¤ 81.00 | â‰¥ 0.5 | â€” | +1pt |
| Public Strong Baseline  | â‰¤ 73.00 | â‰¥ 0.85 | 30 mins | +1pt |
| Private Strong Baseline | â‰¤ 73.00 | â‰¥ 0.85 | â€” | +1pt |

æœ¬æ¬¡ä½œä¸šåªæä¾›æœ¬åœ°ä»£ç ã€‚

# Evalution
fidå’Œpdrï¼Œfidæœ‰ç°æˆçš„åŒ…ï¼Œç›´æ¥è‡ªå·±ä¸‹è½½ç®—åˆ†æ•°å°±å¥½äº†ï¼Œpdræ˜¯å®å¯æ¢¦åˆ†ç±»æ­£ç¡®ç‡ï¼Œä½œä¸šæŒ‡å¯¼æ²¡æŒ‡æ˜æ˜¯å•¥æ¨¡å‹ï¼Œæˆ‘åœ¨ç½‘ä¸Šä¹Ÿæ²¡æ‰¾åˆ°åˆé€‚çš„ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥è‡ªå·±ä¸‹è½½ä¸ªVLæ¨¡å‹æ¥åˆ†ç±»ã€‚
# My approch
è¿™æ¬¡ä½œä¸šè¶…ç®€å•ï¼Œå°±æŠŠæ¨¡å‹æå¤§ä¸€ç‚¹å°±å¥½äº†ï¼ŒæŠŠåŸé…ç½®çš„ä¸¤å±‚æ¢å…­å±‚ï¼Œååˆ†é’Ÿå°±è®­ç»ƒå®Œã€‚fidç«‹é©¬æš´è·Œè‡³å››åå¤šï¼Œè‚‰çœ¼è§‚å¯Ÿå›¾ç‰‡è§‰å¾—ä¹Ÿéƒ½æ˜¯å®å¯æ¢¦ï¼Œæ–™æƒ³pdrä¹Ÿä¸ä¼šä½ã€‚
```python
gpt2_config = {
    "activation_function": "gelu_new",    # Activation function used in the model
    "architectures": ["GPT2LMHeadModel"],  # Specifies the model type
    "attn_pdrop": 0.1,            # Dropout rate for attention layers
    "embd_pdrop": 0.1,            # Dropout rate for embeddings
    "initializer_range": 0.02,        # Standard deviation for weight initialization
    "layer_norm_epsilon": 1e-05,       # Small constant to improve numerical stability in layer norm
    "model_type": "gpt2",           # Type of model
    "n_ctx": 128,               # Context size (maximum sequence length)
    "n_embd": 64,              # Embedding size
    "n_head": 4,               # Number of attention heads
    "n_layer": 6,              # Number of transformer layers
    "n_positions": 400,           # Maximum number of token positions
    "resid_pdrop": 0.1,           # Dropout rate for residual connections
    "vocab_size": num_classes,       # Number of unique tokens in vocabulary
    "pad_token_id": None,          # Padding token ID (None means no padding token)
    "eos_token_id": None,          # End-of-sequence token ID (None means not explicitly defined)
}
```
