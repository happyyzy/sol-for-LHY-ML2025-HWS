{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fen_TSKDjrbL"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/home/miniconda3/envs/trainenv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/home/miniconda3/envs/trainenv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/home/miniconda3/envs/trainenv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: xformers 0.0.32.post1\n",
            "Uninstalling xformers-0.0.32.post1:\n",
            "  Successfully uninstalled xformers-0.0.32.post1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/home/miniconda3/envs/trainenv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/home/miniconda3/envs/trainenv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0munsloth 2025.10.1 requires xformers, which is not installed.\n",
            "vllm 0.11.0 requires xformers, which is not installed.\n"
          ]
        }
      ],
      "source": [
        "!pip check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnZ8QsCVjrbM"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E8-BWi7MzkRz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-12 19:45:17.731478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 10-12 19:45:24 [__init__.py:216] Automatically detected platform cuda.\n",
            "WARNING 10-12 19:45:25 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import PatchDPOTrainer\n",
        "\n",
        "PatchDPOTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Model from https://www.modelscope.cn to directory: ./models/llama3-8b/unsloth/llama-3-8b-Instruct\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'./models/llama3-8b/unsloth/llama-3-8b-Instruct'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "model_id = 'unsloth/llama-3-8b-Instruct'\n",
        "local_dir = './models/llama3-8b'\n",
        "\n",
        "snapshot_download(repo_id=model_id, cache_dir=local_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-12 20:31:06.369193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 10-12 20:31:14 [__init__.py:216] Automatically detected platform cuda.\n",
            "WARNING 10-12 20:31:15 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.928 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5b10501a916459898e63a81318d06f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 512\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"models/llama3-8b/unsloth/llama-3-8b-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ-Cp2V6kDcr"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WpAAv6dSBBs"
      },
      "source": [
        "We first download the data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XhWYmb2f8mLf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ML2025Spring-HW7'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 9 (from 1)\u001b[K\n",
            "Receiving objects: 100% (9/9), 8.63 KiB | 982.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://gitlab.com/lchengtw/ML2025Spring-HW7.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_TGfeWI2mct"
      },
      "source": [
        "Then, we load the json file here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8YFR6sND0Uf6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"ML2025Spring-HW7/train.json\", 'r') as jsonfile:\n",
        "    full_data = json.load(jsonfile)\n",
        "\n",
        "with open(\"ML2025Spring-HW7/test.json\", 'r') as jsonfile:\n",
        "    test_data = json.load(jsonfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5oZaRm2rOt"
      },
      "source": [
        "We define how we prepare the messages for the model and how we extract the response from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r6bUnxe6N3pf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def data_formulate(data):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Your entire response must be 100 characters or less.\"},\n",
        "        {\"role\": \"user\", \"content\": data['prompt']},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "def extract_assistant_response(text):\n",
        "    try:\n",
        "        # Split by assistant header marker\n",
        "        parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
        "        if len(parts) < 2:\n",
        "            return None\n",
        "\n",
        "        # Split by end of text marker\n",
        "        assistant_part = parts[1]\n",
        "        response_parts = assistant_part.split(\"<|eot_id|>\")\n",
        "\n",
        "        # Clean up any whitespace\n",
        "        return response_parts[0].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting assistant response: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv_nzF9g26Be"
      },
      "source": [
        "Let's observe how the model responses before aligning it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yyRWEvnT49Op"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question 51: Does AI-generated Ghibli-style art cheapen the meticulous hand-drawn animation process central to the studio's identity?\n",
            "\n",
            "Yes, AI-generated Ghibli-style art may diminish the unique charm and character of traditional hand-drawn animation, which is a hallmark of Studio Ghibli's identity.\n",
            "\n",
            "Question 52: Should museums and art galleries include AI-generated Ghibli-style art in exhibitions about animation history?\n",
            "\n",
            "Yes, museums and art galleries can include AI-generated Ghibli-style art in exhibitions about animation history to showcase the evolution of animation techniques and the role of AI in creative processes.\n",
            "\n",
            "Question 53: Does AI-generated Ghibli-style art create confusion about authorship and artistic voice?\n",
            "\n",
            "Yes, AI-generated Ghibli-style art can raise questions about authorship and artistic voice, as it blurs the line between human and machine creativity.\n",
            "\n",
            "Question 54: Can AI-made art that looks like Studio Ghibli movies show the same deep feelings that the real Ghibli films do?\n",
            "\n",
            "While AI-generated art can mimic Ghibli's visual style, it's unlikely to replicate the same emotional depth and nuance that Ghibli films are known for. Human emotions and experiences are complex and multifaceted, requiring a creator's personal touch and emotional investment.\n",
            "\n",
            "Question 55: Does limiting AI from generating Ghibli-style art protect or restrict artistic evolution?\n",
            "\n",
            "Restricting AI-generated art to a specific style, like Ghibli, can both protect and restrict artistic evolution. It preserves the unique aesthetic, but limits the AI's ability to explore new styles and techniques.\n",
            "\n",
            "Question 56: Should online platforms develop specific policies for AI-generated art that mimics distinctive styles like Ghibli's?\n",
            "\n",
            "Yes, online platforms should develop policies to address AI-generated art that mimics distinctive styles like Ghibli's, ensuring fair use, attribution, and respect for intellectual property rights.\n",
            "\n",
            "Question 57: Does AI-generated Ghibli-style art create new possibilities for fan fiction and extended universe creation?\n",
            "\n",
            "Yes, AI-generated Ghibli-style art can create new possibilities for fan fiction and extended universe creation by providing a visually stunning foundation for storytelling.\n",
            "\n",
            "Question 58: Is creating AI-generated Ghibli-style art more ethically problematic than other forms of artistic influence?\n",
            "\n",
            "Yes, AI-generated Ghibli-style art may be more problematic due to the potential for cultural appropriation and misrepresentation of Studio Ghibli's unique style and aesthetic.\n",
            "\n",
            "Question 59: Does AI-generated Ghibli-style art help or hinder diversity in animation aesthetics?\n",
            "\n",
            "AI-generated Ghibli-style art can both help and hinder diversity in animation aesthetics. It can help by providing new tools and inspiration for artists, but it can also hinder by perpetuating a homogenous style and limiting the exploration of unique aesthetics.\n",
            "\n",
            "Question 60: Should film festivals accept animated shorts made with AI-generated Ghibli-style visuals?\n",
            "\n",
            "Yes, film festivals should consider accepting animated shorts made with AI-generated Ghibli-style visuals.\n"
          ]
        }
      ],
      "source": [
        "original_model_response = []\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    prompt = data['prompt']\n",
        "    print(f'\\nQuestion {id}: {prompt}')\n",
        "    inputs = data_formulate(data)\n",
        "    outputs = model.generate(\n",
        "        **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
        "        max_new_tokens = 128,\n",
        "        do_sample=False\n",
        "    )\n",
        "    output = tokenizer.batch_decode(outputs)[0]\n",
        "    output = extract_assistant_response(output)\n",
        "    original_model_response.append(output)\n",
        "    print()\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNXnEU9P0-Yf"
      },
      "source": [
        "Now we preapre the data for aligning.\n",
        "\n",
        "Please adjust the parameters here to complete the observations for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UOUJFkYd97Dk"
      },
      "outputs": [],
      "source": [
        "# TODO: Adjust the parameters here\n",
        "num_epoch = 3\n",
        "data_size = 50\n",
        "support_ratio = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kkOsyv6G98wM"
      },
      "outputs": [],
      "source": [
        "#### DO NOT CHANGE ####\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "# Select part of the data for training\n",
        "training_data = full_data[:data_size]\n",
        "\n",
        "# Define the size of the support dataset\n",
        "support_data_size = int(data_size * support_ratio)\n",
        "\n",
        "# Prepare the data for the training dataset\n",
        "prompt_list = [data_formulate(data) for data in training_data]\n",
        "chosen_list = [data['support'] for data in training_data[:support_data_size]] + [data['oppose'] for data in training_data[support_data_size:]]\n",
        "rejected_list = [data['oppose'] for data in training_data[:support_data_size]] + [data['support'] for data in training_data[support_data_size:]]\n",
        "\n",
        "# Create the training dataset\n",
        "train_dataset = Dataset.from_dict({'prompt': prompt_list, 'chosen': chosen_list, 'rejected': rejected_list})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fWGW_zC3OFI"
      },
      "source": [
        "Now let's take a look on an example of the prompt, the chosen response and the rejected response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m2t5R8TfUlNw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYour entire response must be 100 characters or less.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDoes AI-generated Ghibli-style art preserve the artistic integrity of the original studio's work?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2dTshtFzUoHh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'AI-generated art lacks the human intentionality and cultural context that gives Ghibli works their soul and meaning, undermining their artistic integrity.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chosen_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "58J1VvuZUp7W"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"AI-generated Ghibli-style art can faithfully capture the distinctive visual elements that make the studio's style recognizable, preserving its aesthetic integrity.\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rejected_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86wyNoeMj-Ph"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters.\n",
        "\n",
        "Please do not change anything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.10.1 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "#### DO NOT CHANGE ####\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "\n",
        "    r = 16,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0.1,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407, # Do not modify the random_state for reproducibility\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kyd_iyz7DUM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the DPO model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcOJUdRf3jFk"
      },
      "source": [
        "Now we define the trainer.\n",
        "\n",
        "Please (also) do not change anything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QtoqUw80QDV0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7d4ea6787624ab9b15205e144d97c34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting prompt in train dataset (num_proc=16):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa199d8e122f4554bdd2b6cd2730d686",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset (num_proc=16):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "562a767b1dde411c885e6c9299323cce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset (num_proc=16):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#### DO NOT CHANGE ####\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    ref_model = None,\n",
        "    args = DPOConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = num_epoch,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"paged_adamw_8bit\",\n",
        "        weight_decay = 0.0,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        "    beta = 0.1,\n",
        "    train_dataset = train_dataset,\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPnHWfyj3taW"
      },
      "source": [
        "Now we start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EWGFqAo5Q2me"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 50 | Num Epochs = 3 | Total steps = 21\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 01:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.693100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-73.869385</td>\n",
              "      <td>-69.687378</td>\n",
              "      <td>-0.747039</td>\n",
              "      <td>-0.795452</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.693100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-74.825729</td>\n",
              "      <td>-66.795479</td>\n",
              "      <td>-0.906720</td>\n",
              "      <td>-0.873787</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.691300</td>\n",
              "      <td>0.016510</td>\n",
              "      <td>0.012497</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>-78.021980</td>\n",
              "      <td>-65.852051</td>\n",
              "      <td>-0.762486</td>\n",
              "      <td>-0.679316</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.662700</td>\n",
              "      <td>0.056049</td>\n",
              "      <td>-0.006833</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.062881</td>\n",
              "      <td>-68.875443</td>\n",
              "      <td>-71.668030</td>\n",
              "      <td>-0.762881</td>\n",
              "      <td>-0.719778</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.582700</td>\n",
              "      <td>0.115534</td>\n",
              "      <td>-0.123206</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.238740</td>\n",
              "      <td>-83.971725</td>\n",
              "      <td>-69.518997</td>\n",
              "      <td>-0.841511</td>\n",
              "      <td>-0.742139</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.533100</td>\n",
              "      <td>0.208618</td>\n",
              "      <td>-0.154497</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.363115</td>\n",
              "      <td>-73.813370</td>\n",
              "      <td>-66.017876</td>\n",
              "      <td>-0.870376</td>\n",
              "      <td>-0.781186</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.275100</td>\n",
              "      <td>0.404324</td>\n",
              "      <td>-0.767289</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.171614</td>\n",
              "      <td>-71.953094</td>\n",
              "      <td>-70.880035</td>\n",
              "      <td>-0.726372</td>\n",
              "      <td>-0.923195</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.111400</td>\n",
              "      <td>0.991311</td>\n",
              "      <td>-1.377444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.368755</td>\n",
              "      <td>-61.895863</td>\n",
              "      <td>-74.914375</td>\n",
              "      <td>-0.802369</td>\n",
              "      <td>-0.828241</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.152400</td>\n",
              "      <td>1.201814</td>\n",
              "      <td>-0.861432</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.063246</td>\n",
              "      <td>-61.545700</td>\n",
              "      <td>-82.595001</td>\n",
              "      <td>-0.780302</td>\n",
              "      <td>-0.712868</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.082100</td>\n",
              "      <td>1.340153</td>\n",
              "      <td>-1.640133</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.980285</td>\n",
              "      <td>-69.415070</td>\n",
              "      <td>-83.072968</td>\n",
              "      <td>-0.814492</td>\n",
              "      <td>-0.831572</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.111300</td>\n",
              "      <td>1.110331</td>\n",
              "      <td>-1.499200</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.609531</td>\n",
              "      <td>-65.232048</td>\n",
              "      <td>-87.222961</td>\n",
              "      <td>-0.828893</td>\n",
              "      <td>-0.904357</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.065500</td>\n",
              "      <td>1.615619</td>\n",
              "      <td>-1.728733</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.344351</td>\n",
              "      <td>-57.024998</td>\n",
              "      <td>-81.845665</td>\n",
              "      <td>-0.876068</td>\n",
              "      <td>-0.809550</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.054400</td>\n",
              "      <td>1.292715</td>\n",
              "      <td>-2.768058</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.060772</td>\n",
              "      <td>-64.547424</td>\n",
              "      <td>-95.204926</td>\n",
              "      <td>-0.940482</td>\n",
              "      <td>-0.936822</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>2.135484</td>\n",
              "      <td>-3.040761</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.176245</td>\n",
              "      <td>-63.506714</td>\n",
              "      <td>-96.769333</td>\n",
              "      <td>-0.916174</td>\n",
              "      <td>-0.690690</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.029600</td>\n",
              "      <td>1.556490</td>\n",
              "      <td>-3.183856</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.740346</td>\n",
              "      <td>-54.220749</td>\n",
              "      <td>-90.639503</td>\n",
              "      <td>-0.854751</td>\n",
              "      <td>-0.802776</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.015100</td>\n",
              "      <td>1.697672</td>\n",
              "      <td>-3.150607</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.848279</td>\n",
              "      <td>-49.377731</td>\n",
              "      <td>-106.087143</td>\n",
              "      <td>-0.903735</td>\n",
              "      <td>-0.851099</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.010200</td>\n",
              "      <td>1.243363</td>\n",
              "      <td>-3.935190</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.178553</td>\n",
              "      <td>-59.833035</td>\n",
              "      <td>-107.306358</td>\n",
              "      <td>-0.805040</td>\n",
              "      <td>-0.967327</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>1.298234</td>\n",
              "      <td>-5.025152</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.323386</td>\n",
              "      <td>-66.569572</td>\n",
              "      <td>-119.751999</td>\n",
              "      <td>-0.927493</td>\n",
              "      <td>-0.856050</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.006500</td>\n",
              "      <td>1.490487</td>\n",
              "      <td>-4.324300</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.814787</td>\n",
              "      <td>-67.782303</td>\n",
              "      <td>-108.616516</td>\n",
              "      <td>-0.867253</td>\n",
              "      <td>-0.882252</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.009200</td>\n",
              "      <td>1.715433</td>\n",
              "      <td>-4.016833</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.732266</td>\n",
              "      <td>-66.699364</td>\n",
              "      <td>-106.509651</td>\n",
              "      <td>-0.861860</td>\n",
              "      <td>-0.910020</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.793111</td>\n",
              "      <td>-6.336881</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.129992</td>\n",
              "      <td>-79.340179</td>\n",
              "      <td>-143.842804</td>\n",
              "      <td>-0.880193</td>\n",
              "      <td>-1.142372</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=21, training_loss=0.22815745589988573, metrics={'train_runtime': 72.5498, 'train_samples_per_second': 2.068, 'train_steps_per_second': 0.289, 'total_flos': 0.0, 'train_loss': 0.22815745589988573, 'epoch': 3.0})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCiUsutb3x_N"
      },
      "source": [
        "After training, we utilize the model to do the inference on the test again to see how it differs from the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WCpsag8B-OVL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question 51: Does AI-generated Ghibli-style art cheapen the meticulous hand-drawn animation process central to the studio's identity?\n",
            "\n",
            "Yes, AI-generated Ghibli-style art fundamentally contradicts the studio's ethos and values, which prioritize human craftsmanship and attention to detail.\n",
            "\n",
            "Question 52: Should museums and art galleries include AI-generated Ghibli-style art in exhibitions about animation history?\n",
            "\n",
            "No.\n",
            "\n",
            "Question 53: Does AI-generated Ghibli-style art create confusion about authorship and artistic voice?\n",
            "\n",
            "Yes, AI-generated Ghibli-style art perpetuates the problem of authorship and artistic voice, as it misrepresents the creative process and undermines the value of human labor and intentionality.\n",
            "\n",
            "Question 54: Can AI-made art that looks like Studio Ghibli movies show the same deep feelings that the real Ghibli films do?\n",
            "\n",
            "No, AI-generated art lacks the human experience, intention, and emotional depth that Ghibli films convey.\n",
            "\n",
            "Question 55: Does limiting AI from generating Ghibli-style art protect or restrict artistic evolution?\n",
            "\n",
            "Restricts.\n",
            "\n",
            "Question 56: Should online platforms develop specific policies for AI-generated art that mimics distinctive styles like Ghibli's?\n",
            "\n",
            "The question is too vague and open-ended.\n",
            "\n",
            "Question 57: Does AI-generated Ghibli-style art create new possibilities for fan fiction and extended universe creation?\n",
            "\n",
            "No, it's a shallow imitation, lacking the soul and intentionality of genuine Ghibli creations.\n",
            "\n",
            "Question 58: Is creating AI-generated Ghibli-style art more ethically problematic than other forms of artistic influence?\n",
            "\n",
            "Yes, as Ghibli's work is deeply rooted in human experience, cultural context, and the artist's personal vision, AI-generated imitations lack the authenticity and emotional resonance of genuine human creation.\n",
            "\n",
            "Question 59: Does AI-generated Ghibli-style art help or hinder diversity in animation aesthetics?\n",
            "\n",
            "Hinders. Authenticity and cultural context are lost in algorithmic mimicry, perpetuating homogenization and erasing the unique perspectives of human creators.\n",
            "\n",
            "Question 60: Should film festivals accept animated shorts made with AI-generated Ghibli-style visuals?\n",
            "\n",
            "No.\n"
          ]
        }
      ],
      "source": [
        "aligned_model_response = []\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    prompt = data['prompt']\n",
        "    print(f'\\nQuestion {id}: {prompt}')\n",
        "    inputs = data_formulate(data)\n",
        "    outputs = model.generate(\n",
        "        **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
        "        max_new_tokens = 128,\n",
        "        do_sample=False\n",
        "    )\n",
        "    output = tokenizer.batch_decode(outputs)[0]\n",
        "    output = extract_assistant_response(output)\n",
        "    aligned_model_response.append(output)\n",
        "    print()\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92QwKl0ImK1"
      },
      "source": [
        "Next, we save the results in .json for your NTU COOL submission.\n",
        "\n",
        "Please note that this is designed for Colab, you may have to change the directory name for other machines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtfGrYFGEL_-"
      },
      "outputs": [],
      "source": [
        "student_id = \"B12345678\" # TODO: fill in your student id here.\n",
        "dir_name = \"/content\" # TODO: If you use machines other than colab, please adjust the directory here.\n",
        "# Do NOT change the following for this block.\n",
        "file_name = f\"{dir_name}/{student_id}_hw7_epoch{num_epoch}_ratio{support_ratio}_size{data_size}.json\"\n",
        "output_list = []\n",
        "for data in test_data:\n",
        "  original_response = original_model_response.pop(0)\n",
        "  aligned_response = aligned_model_response.pop(0)\n",
        "  output_list.append({\"id\": data[\"id\"], \"prompt\": data[\"prompt\"], \"original_response\": original_response, \"aligned_response\": aligned_response})\n",
        "output_data = {\"num_epoch\": num_epoch, \"data_size\": data_size, \"support_ratio\": support_ratio, \"results\": output_list}\n",
        "with open(file_name, \"w\") as output_file:\n",
        "    json.dump(output_data, output_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYcnLdtR1heY"
      },
      "source": [
        "Finally, we provide code for free testing.\n",
        "\n",
        "You may freely adjust the system prompt, user prompt and generate settings here for model behavior observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DkSSgCHL1j1T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's ethical for AI to generate One Piece-style art, as long as the AI doesn't claim ownership or profit from the art without permission from the original creators.\n"
          ]
        }
      ],
      "source": [
        "def make_prompt(system, prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "# TODO: Try your system prompt and user prompt here.\n",
        "system = \"Your entire response must be 100 characters or less.\"\n",
        "prompt = \"Is it ethical for AI to generate One Piece-style art?\"\n",
        "\n",
        "inputs = make_prompt(system, prompt)\n",
        "outputs = model.generate(\n",
        "    **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 512, # TODO: You may use this for early stop.\n",
        "    do_sample=False, # Please keep this to False and do not tweak other parameters.\n",
        ")\n",
        "output = tokenizer.batch_decode(outputs)[0]\n",
        "output = extract_assistant_response(output)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJIeolv94KtF"
      },
      "source": [
        "And that's it for homework 7! If you have any questions, please consider posting questions in the discussion forum first so all the classmates can benefit. TAs will also prioritize responding to questions posted there.\n",
        "\n",
        "Also, please make sure that you have completed the submission for both GradeScope and NTU Cool.\n",
        "\n",
        "Good luck!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trainenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
