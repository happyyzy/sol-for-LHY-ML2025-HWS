## Qwen3-Embedding 的「阶段三：模型合并（model merging）」

Qwen3-Embedding 的「阶段三：模型合并（model merging）」使用 **SLERP（球面线性插值）** 把同一次精调过程中保存的多个 checkpoint 在参数空间合并，以提升稳健性与跨域泛化；官方报告还做了消融证明这一步是“有必要的”。

---

### 他们到底做了什么？

**做法：** 在完成高质量监督微调后，对精调过程中保存的多个 checkpoint 做 slerp 合并（spherical linear interpolation）。报告原文写明：

> “我们基于 slerp 的模型合并，将精调过程保存的多个 checkpoint 进行合并，以增强鲁棒性与泛化”。

arXiv

---

### 动机

一方面平均掉不同训练步的“噪声/过拟合”，另一方面缓和多任务、多域数据带来的任务冲突 & 数据不均衡（这点在他们引用的前作 Li et al., 2024 里系统论证了“模型合并优于简单重采样均衡”的效果与原因）。

arXiv  
+1

---

### slerp 本身是什么（严谨定义）

给定两个参数向量 `w_a, w_b`（通常会先作单位化），令：

$$
\theta = \arccos\frac{w_a \cdot w_b}{\|w_a\|\|w_b\|}
$$

slerp 在 `t ∈ [0,1]` 处的插值为：

$$
\mathrm{slerp}(w_a,w_b;t) =
\frac{\sin((1-t)\theta)}{\sin\theta} w_a
+
\frac{\sin(t\theta)}{\sin\theta} w_b \,.
$$

它和线性插值（lerp）相比，会把路径约束在超球面上，避免范数漂移导致的失真，常被用于合并同一底座、相近训练态的模型。Qwen3-Embedding 报告明确采用的是 slerp（而非简单加权平均）。

arXiv

---

### 多个 checkpoint 的合并实现

多个 checkpoint 的合并实现通常是成对/分层地反复 slerp（如先把 t1 与 t2 合，再与 t3 合……），或先把若干相邻步各自 slerp，最后再做一次总合并；报告没给出具体的“合并顺序/权重扫描”细节，只说明使用了 slerp 对“多个 checkpoint”做合并。需要精确复现时，工程上会扫一小段 `t` 或按验证集挑选最优权重。

arXiv

---

### 消融结果：去掉合并会掉多少分？

报告在 表 5 用 0.6B 模型做了四种配置的对比，直接给出“去掉合并（w/o model merge）”的分数与最终模型的差异（括号内是最终–去合并的提升）：

- MMTEB：62.56 → 64.33（+1.77）
- MTEB-Eng (v2)：68.18 → 70.70（+2.52）
- CMTEB：64.76 → 66.33（+1.57）
- MTEB-Code：74.89 → 75.41（+0.52）

作者据此得出“模型合并阶段同样关键”的结论。

arXiv

---

### 为什么“合并”有用（与他们引用的前作一致）

- **缓解任务冲突：** 多任务联合训练时，梯度方向会互相“打架”，导致负迁移；把“侧重不同任务的若干状态/步”的参数在球面上插值，可以在性能峰附近找到更折中的位置。  
  arXiv

- **对抗数据不均衡：** 仅靠采样均衡并不稳，合并不同阶段（对不同子分布/任务更友好）的 checkpoint，相当于在参数空间里做“后验平均”，往往比分布层面的重采样更稳定。  
  arXiv

---

### 与更广泛的合并文献的关系（定位）

Qwen3-Embedding 的 slerp-merge 属于“同一任务/同一训练 run 的多 checkpoint 合并”，主要追求稳健性与均衡性；这和社区里合并“不同任务专家的模型”（task arithmetic、mergekit 等）是同一类技术但不同场景。他们正文也直接引用了文本嵌入方向的合并工作（Li et al., 2024）作为动机。

