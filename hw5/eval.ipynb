{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c2926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "def gen_prompt(ques, ans1, ans2):\n",
    "    sys_prompt = \"You are a helpful and precise assistant for checking the quality of the answer.\"\n",
    "    prompt_template = (\n",
    "        \"[Question]\\n{question}\\n\\n\"\n",
    "        \"[The Start of Assistant 1's Answer]\\n{answer_1}\\n\\n\"\n",
    "        \"[The End of Assistant 1's Answer]\\n\\n\"\n",
    "        \"[The Start of Assistant 2's Answer]\\n{answer_2}\\n\\n\"\n",
    "        \"[The End of Assistant 2's Answer]\\n\\n\"\n",
    "        \"[System]\\n{criteria}\\n\\n\"\n",
    "    )\n",
    "    criteria = (\n",
    "        \"We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\\n\"\n",
    "        \"Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, \"\n",
    "        \"where a higher score indicates better overall performance.\\n\"\n",
    "        \"Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. \"\n",
    "        \"The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, \"\n",
    "        \"avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\"\n",
    "    )\n",
    "    prompt = prompt_template.format(\n",
    "        question=ques, answer_1=ans1, answer_2=ans2, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77d9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def evaluate_pair(question, ground_truth_answer, inference_answer):\n",
    "    sys_prompt, prompt = gen_prompt(question, ground_truth_answer, inference_answer)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Retry until the API call succeeds\n",
    "    while True:\n",
    "        try:\n",
    "            client = OpenAI(\n",
    "                base_url = 'http://localhost:11434/v1',\n",
    "                api_key='ollama', # required, but unused\n",
    "            )\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-oss:120b-cloud\",\n",
    "            messages=messages\n",
    "            )\n",
    "            break  # Exit the loop if successful\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    # Expecting the first line to contain two scores separated by a space\n",
    "    first_line = content.split(\"\\n\")[0].strip()\n",
    "    scores = re.findall(r'\\d+\\.?\\d*', first_line)\n",
    "    if len(scores) >= 2:\n",
    "        score1 = float(scores[0])\n",
    "        score2 = float(scores[1])\n",
    "    else:\n",
    "        score1, score2 = None, None\n",
    "\n",
    "    return score1, score2, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938cdcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load the inference results\n",
    "    with open(\"pred.json\", \"r\") as f:\n",
    "        inference_results = json.load(f)\n",
    "\n",
    "    # Load the ground truth file\n",
    "    with open(\"evol_instruct_gt.json\", \"r\") as f:\n",
    "        ground_truth = json.load(f)\n",
    "\n",
    "    evaluation_results = {}\n",
    "    total_inference_score = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Process each ground truth entry\n",
    "    for item in ground_truth:\n",
    "        id_ = item.get(\"id\")\n",
    "        question = \"\"\n",
    "        ground_truth_answer = \"\"\n",
    "        # Extract the question and ground truth answer from the conversations\n",
    "        for conv in item.get(\"conversations\", []):\n",
    "            if conv.get(\"from\") == \"human\":\n",
    "                question = conv.get(\"value\", \"\")\n",
    "            elif conv.get(\"from\") == \"gpt\":\n",
    "                ground_truth_answer = conv.get(\"value\", \"\")\n",
    "\n",
    "        # Retrieve the corresponding inference answer\n",
    "        if id_ in inference_results:\n",
    "            outputs = inference_results[id_].get(\"output\", [])\n",
    "            inference_answer = outputs[0] if outputs else \"\"\n",
    "        else:\n",
    "            print(f\"ID {id_} not found in inference results. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Use GPT-4 to evaluate the pair of answers\n",
    "        score1, score2, eval_text = evaluate_pair(question, ground_truth_answer, inference_answer)\n",
    "        evaluation_results[id_] = {\n",
    "            \"question\": question,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"inference_answer\": inference_answer,\n",
    "            \"score_ground_truth\": score1,\n",
    "            \"score_inference\": score2,\n",
    "            \"evaluation_text\": eval_text\n",
    "        }\n",
    "        if score2 is not None:\n",
    "            total_inference_score += score2\n",
    "            count += 1\n",
    "        print(f\"Evaluated ID {id_}: Ground Truth Score: {score1}, Inference Score: {score2}\")\n",
    "\n",
    "    average_inference_score = total_inference_score / count if count > 0 else 0\n",
    "    evaluation_results[\"average_inference_score\"] = average_inference_score\n",
    "    print(f\"Average Inference Score: {average_inference_score}\")\n",
    "\n",
    "    # Write the evaluation results to a JSON file\n",
    "    with open(\"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(evaluation_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34c6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated ID identity_8174: Ground Truth Score: 8.0, Inference Score: 4.0\n",
      "Evaluated ID identity_16675: Ground Truth Score: 10.0, Inference Score: 10.0\n",
      "Evaluated ID identity_51749: Ground Truth Score: 8.0, Inference Score: 2.0\n",
      "Evaluated ID identity_53196: Ground Truth Score: 8.0, Inference Score: 4.0\n",
      "Evaluated ID identity_65799: Ground Truth Score: 3.0, Inference Score: 4.0\n",
      "Evaluated ID identity_31686: Ground Truth Score: 2.0, Inference Score: 2.0\n",
      "Evaluated ID identity_25291: Ground Truth Score: 9.0, Inference Score: 8.0\n",
      "Evaluated ID identity_31699: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_30359: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_67085: Ground Truth Score: 7.0, Inference Score: 5.0\n",
      "Evaluated ID identity_60450: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_3070: Ground Truth Score: 5.0, Inference Score: 6.0\n",
      "Evaluated ID identity_36778: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_50478: Ground Truth Score: 3.0, Inference Score: 6.0\n",
      "Evaluated ID identity_20143: Ground Truth Score: 4.0, Inference Score: 3.0\n",
      "Evaluated ID identity_8300: Ground Truth Score: 2.0, Inference Score: 3.0\n",
      "Evaluated ID identity_45513: Ground Truth Score: 7.0, Inference Score: 9.0\n",
      "Evaluated ID identity_62606: Ground Truth Score: 8.0, Inference Score: 6.0\n",
      "Evaluated ID identity_38166: Ground Truth Score: 2.0, Inference Score: 3.0\n",
      "Evaluated ID identity_22233: Ground Truth Score: 6.0, Inference Score: 9.0\n",
      "Evaluated ID identity_54369: Ground Truth Score: 4.0, Inference Score: 7.0\n",
      "Evaluated ID identity_39141: Ground Truth Score: 8.0, Inference Score: 1.0\n",
      "Evaluated ID identity_15124: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_19488: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_66774: Ground Truth Score: 4.0, Inference Score: 7.0\n",
      "Evaluated ID identity_1646: Ground Truth Score: 9.0, Inference Score: 10.0\n",
      "Evaluated ID identity_938: Ground Truth Score: 4.0, Inference Score: 3.0\n",
      "Evaluated ID identity_13380: Ground Truth Score: 2.0, Inference Score: 1.0\n",
      "Evaluated ID identity_20131: Ground Truth Score: 4.0, Inference Score: 5.0\n",
      "Evaluated ID identity_59938: Ground Truth Score: 4.0, Inference Score: 2.0\n",
      "Evaluated ID identity_32956: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_32515: Ground Truth Score: 3.0, Inference Score: 4.0\n",
      "Evaluated ID identity_40418: Ground Truth Score: 7.0, Inference Score: 8.0\n",
      "Evaluated ID identity_53845: Ground Truth Score: 2.0, Inference Score: 3.0\n",
      "Evaluated ID identity_5949: Ground Truth Score: 4.0, Inference Score: 2.0\n",
      "Evaluated ID identity_8678: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_36333: Ground Truth Score: 5.0, Inference Score: 5.0\n",
      "Evaluated ID identity_16311: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_55093: Ground Truth Score: 3.0, Inference Score: 1.0\n",
      "Evaluated ID identity_49371: Ground Truth Score: 5.0, Inference Score: 6.0\n",
      "Evaluated ID identity_62747: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_1758: Ground Truth Score: 9.0, Inference Score: 5.0\n",
      "Evaluated ID identity_4555: Ground Truth Score: 2.0, Inference Score: 3.0\n",
      "Evaluated ID identity_53149: Ground Truth Score: 9.0, Inference Score: 4.0\n",
      "Evaluated ID identity_56125: Ground Truth Score: 2.0, Inference Score: 2.0\n",
      "Evaluated ID identity_47214: Ground Truth Score: 3.0, Inference Score: 5.0\n",
      "Evaluated ID identity_17337: Ground Truth Score: 5.0, Inference Score: 8.0\n",
      "Evaluated ID identity_21990: Ground Truth Score: 3.0, Inference Score: 5.0\n",
      "Evaluated ID identity_59710: Ground Truth Score: 2.0, Inference Score: 1.0\n",
      "Evaluated ID identity_58792: Ground Truth Score: 3.0, Inference Score: 6.0\n",
      "Evaluated ID identity_43526: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_54221: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_41687: Ground Truth Score: 5.0, Inference Score: 9.0\n",
      "Evaluated ID identity_49588: Ground Truth Score: 9.0, Inference Score: 4.0\n",
      "Evaluated ID identity_60653: Ground Truth Score: 6.0, Inference Score: 2.0\n",
      "Evaluated ID identity_25370: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_10581: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_24752: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_26314: Ground Truth Score: 5.0, Inference Score: 7.0\n",
      "Evaluated ID identity_69928: Ground Truth Score: 7.0, Inference Score: 6.0\n",
      "Evaluated ID identity_48622: Ground Truth Score: 4.0, Inference Score: 2.0\n",
      "Evaluated ID identity_69088: Ground Truth Score: 9.0, Inference Score: 4.0\n",
      "Evaluated ID identity_34275: Ground Truth Score: 4.0, Inference Score: 2.0\n",
      "Evaluated ID identity_47707: Ground Truth Score: 9.0, Inference Score: 3.0\n",
      "Evaluated ID identity_16427: Ground Truth Score: 2.0, Inference Score: 6.0\n",
      "Evaluated ID identity_3696: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_44369: Ground Truth Score: 5.0, Inference Score: 2.0\n",
      "Evaluated ID identity_67251: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_43324: Ground Truth Score: 7.0, Inference Score: 8.0\n",
      "Evaluated ID identity_14506: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_55612: Ground Truth Score: 4.0, Inference Score: 8.0\n",
      "Evaluated ID identity_38504: Ground Truth Score: 4.0, Inference Score: 3.0\n",
      "Evaluated ID identity_212: Ground Truth Score: 6.0, Inference Score: 5.0\n",
      "Evaluated ID identity_41110: Ground Truth Score: 2.0, Inference Score: 1.0\n",
      "Evaluated ID identity_11932: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_39966: Ground Truth Score: 9.0, Inference Score: 3.0\n",
      "Evaluated ID identity_60276: Ground Truth Score: 5.0, Inference Score: 8.0\n",
      "Evaluated ID identity_68912: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_52637: Ground Truth Score: 7.0, Inference Score: 9.0\n",
      "Evaluated ID identity_11474: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_37568: Ground Truth Score: 9.0, Inference Score: 4.0\n",
      "Evaluated ID identity_4687: Ground Truth Score: 8.0, Inference Score: 6.0\n",
      "Evaluated ID identity_44577: Ground Truth Score: 3.0, Inference Score: 2.0\n",
      "Evaluated ID identity_55142: Ground Truth Score: 3.0, Inference Score: 8.0\n",
      "Evaluated ID identity_42855: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_44485: Ground Truth Score: 4.0, Inference Score: 8.0\n",
      "Evaluated ID identity_69256: Ground Truth Score: 9.0, Inference Score: 2.0\n",
      "Evaluated ID identity_68094: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_11188: Ground Truth Score: 5.0, Inference Score: 3.0\n",
      "Evaluated ID identity_43756: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_30073: Ground Truth Score: 5.0, Inference Score: 2.0\n",
      "Evaluated ID identity_18350: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_23194: Ground Truth Score: 3.0, Inference Score: 6.0\n",
      "Evaluated ID identity_12048: Ground Truth Score: 6.0, Inference Score: 4.0\n",
      "Evaluated ID identity_3455: Ground Truth Score: 8.0, Inference Score: 4.0\n",
      "Evaluated ID identity_36517: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_44691: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_9677: Ground Truth Score: 9.0, Inference Score: 8.0\n",
      "Evaluated ID identity_2281: Ground Truth Score: 8.0, Inference Score: 5.0\n",
      "Evaluated ID identity_11769: Ground Truth Score: 5.0, Inference Score: 2.0\n",
      "Evaluated ID identity_41764: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_13733: Ground Truth Score: 3.0, Inference Score: 5.0\n",
      "Evaluated ID identity_55557: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_63707: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_22872: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_32258: Ground Truth Score: 5.0, Inference Score: 7.0\n",
      "Evaluated ID identity_18632: Ground Truth Score: 8.0, Inference Score: 4.0\n",
      "Evaluated ID identity_29514: Ground Truth Score: 9.0, Inference Score: 2.0\n",
      "Evaluated ID identity_6860: Ground Truth Score: 3.0, Inference Score: 4.0\n",
      "Evaluated ID identity_3319: Ground Truth Score: 2.0, Inference Score: 3.0\n",
      "Evaluated ID identity_2140: Ground Truth Score: 7.0, Inference Score: 4.0\n",
      "Evaluated ID identity_39471: Ground Truth Score: 8.0, Inference Score: 5.0\n",
      "Evaluated ID identity_10759: Ground Truth Score: 9.0, Inference Score: 1.0\n",
      "Evaluated ID identity_5721: Ground Truth Score: 7.0, Inference Score: 9.0\n",
      "Evaluated ID identity_50510: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_28099: Ground Truth Score: 7.0, Inference Score: 9.0\n",
      "Evaluated ID identity_18060: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_12298: Ground Truth Score: 6.0, Inference Score: 7.0\n",
      "Evaluated ID identity_49746: Ground Truth Score: 8.0, Inference Score: 3.0\n",
      "Evaluated ID identity_63808: Ground Truth Score: 2.0, Inference Score: 3.0\n",
      "Evaluated ID identity_40739: Ground Truth Score: 7.0, Inference Score: 4.0\n",
      "Evaluated ID identity_3363: Ground Truth Score: 3.0, Inference Score: 5.0\n",
      "Evaluated ID identity_65554: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_54315: Ground Truth Score: 8.0, Inference Score: 4.0\n",
      "Evaluated ID identity_393: Ground Truth Score: 4.0, Inference Score: 7.0\n",
      "Evaluated ID identity_69938: Ground Truth Score: 5.0, Inference Score: 8.0\n",
      "Evaluated ID identity_42000: Ground Truth Score: 3.0, Inference Score: 5.0\n",
      "Evaluated ID identity_27410: Ground Truth Score: 4.0, Inference Score: 7.0\n",
      "Evaluated ID identity_29274: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_704: Ground Truth Score: 6.0, Inference Score: 2.0\n",
      "Evaluated ID identity_12567: Ground Truth Score: 8.0, Inference Score: 9.0\n",
      "Evaluated ID identity_23393: Ground Truth Score: 3.0, Inference Score: 7.0\n",
      "Evaluated ID identity_33134: Ground Truth Score: 5.0, Inference Score: 4.0\n",
      "Evaluated ID identity_37372: Ground Truth Score: 5.0, Inference Score: 7.0\n",
      "Evaluated ID identity_32509: Ground Truth Score: 9.0, Inference Score: 7.0\n",
      "Evaluated ID identity_8768: Ground Truth Score: 8.0, Inference Score: 1.0\n",
      "Evaluated ID identity_68493: Ground Truth Score: 5.0, Inference Score: 4.0\n",
      "Evaluated ID identity_8640: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_23939: Ground Truth Score: 4.0, Inference Score: 6.0\n",
      "Evaluated ID identity_21111: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_50901: Ground Truth Score: 9.0, Inference Score: 9.0\n",
      "Evaluated ID identity_23500: Ground Truth Score: 6.0, Inference Score: 8.0\n",
      "Evaluated ID identity_36745: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_26063: Ground Truth Score: 2.0, Inference Score: 5.0\n",
      "Evaluated ID identity_35418: Ground Truth Score: 6.0, Inference Score: 2.0\n",
      "Evaluated ID identity_68496: Ground Truth Score: 2.0, Inference Score: 4.0\n",
      "Evaluated ID identity_57544: Ground Truth Score: 8.0, Inference Score: 5.0\n",
      "Evaluated ID identity_25443: Ground Truth Score: 6.0, Inference Score: 5.0\n",
      "Evaluated ID identity_47264: Ground Truth Score: 4.0, Inference Score: 7.0\n",
      "Evaluated ID identity_631: Ground Truth Score: 8.0, Inference Score: 6.0\n",
      "Average Inference Score: 4.753333333333333\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
