{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FOLEQCjGRCX"
   },
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj1bnkewjzjx"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
    "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install unsloth==2025.2.15 unsloth_zoo==2025.2.7\n",
    "!pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ericsunkuan/ML_Spring2025_HW5.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPfTaySmjzjz"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVqNG3cME9r1"
   },
   "source": [
    "#### Note : Changing the model is against the Rules of Homework 5 !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EadFCbszFUkK"
   },
   "source": [
    "### Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-28 15:57:58 [__init__.py:216] Automatically detected platform cuda.\n",
      "WARNING 09-28 15:57:58 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.7: Fast Llama patching. Transformers: 4.55.4. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.928 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "### Changing the model here is forbidden !\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"llama2-7b-bnb-4bit\",    ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "Add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.9.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 12, ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha = 16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "\n",
    "\n",
    "################# TODO  ####################################################################\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",  ### Use llama-3.1 template for better performance here\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48fl3jE0UC-i"
   },
   "source": [
    "# Dataset Preperation (Loading and Refining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLAk-ZCOUP1x"
   },
   "source": [
    "## Data Filtering & Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'conversations', 'score', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n",
      "\n",
      "Top examples sorted by simple conversation length:\n",
      "ID: identity_23488, Conversation Length: 22\n",
      "ID: identity_717, Conversation Length: 23\n",
      "ID: identity_1347, Conversation Length: 23\n",
      "ID: identity_1790, Conversation Length: 23\n",
      "ID: identity_2502, Conversation Length: 23\n",
      "\n",
      "Top examples sorted by advanced key (combination of conversation length and score):\n",
      "ID: identity_23370, Advanced Key Value: 5.00049\n",
      "ID: identity_37225, Advanced Key Value: 5.00047\n",
      "ID: identity_40362, Advanced Key Value: 5.0004\n",
      "ID: identity_8015, Advanced Key Value: 5.00037\n",
      "ID: identity_30813, Advanced Key Value: 5.00037\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_from_disk(\"ML_Spring2025_HW5/fastchat_alpaca_52k\")\n",
    "\n",
    "# ---------------------------\n",
    "# Add a \"text\" field to each example\n",
    "# ---------------------------\n",
    "# This function extracts the first assistant message from the conversation\n",
    "def add_text_field(example):\n",
    "    # Extract the first message where role == 'assistant'\n",
    "    assistant_texts = [msg[\"content\"] for msg in example[\"conversations\"] if msg[\"role\"] == \"assistant\"]\n",
    "    text = assistant_texts[0] if assistant_texts else \"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Map the function over the dataset to add the \"text\" column.\n",
    "dataset = dataset.map(add_text_field)\n",
    "\n",
    "# Print the dataset structure to confirm the new feature.\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#################### TODO : Define a helper function for computing conversation length ###############\n",
    "\n",
    "# The default \"conversation length\" here refers to the length of the input (human) and output (gpt), you can modify it at your will\n",
    "\n",
    "def compute_conversation_length(example):\n",
    "    # Compute total word count across all messages in the 'conversations' field\n",
    "    return sum(len(message[\"content\"].split()) for message in example[\"conversations\"])\n",
    "\n",
    "\n",
    "#################### TODO ############################################################################\n",
    "\n",
    "# ---------------------------\n",
    "# Simple Sorting Method  (Default)\n",
    "# ---------------------------\n",
    "# Sort the dataset from shortest to longest conversation (by word count)\n",
    "sorted_dataset_simple_list = sorted(dataset, key=compute_conversation_length, reverse=False)\n",
    "\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_simple = Dataset.from_list(sorted_dataset_simple_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by simple conversation length:\")\n",
    "for entry in sorted_dataset_simple.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Conversation Length: {compute_conversation_length(entry)}\")\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "\n",
    "############## Advanced Sorting Method (TODO : Modify the sorting key ##################\n",
    "# ---------------------------\n",
    "# Default : Sorting based on Combining conversation length with the 'score' field using a weighted sum.\n",
    "# Here, we multiply the score by 10 and add it to the conversation length.\n",
    "def advanced_sort_key(example):\n",
    "    conversation_len = compute_conversation_length(example)\n",
    "    score = example[\"score\"]\n",
    "    return 1e-5 * conversation_len + score * 1\n",
    "\n",
    "####################################### TODO ###########################################\n",
    "\n",
    "sorted_dataset_advanced_list = sorted(dataset, key=advanced_sort_key, reverse=True)\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_advanced = Dataset.from_list(sorted_dataset_advanced_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by advanced key (combination of conversation length and score):\")\n",
    "for entry in sorted_dataset_advanced.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Advanced Key Value: {advanced_sort_key(entry)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BVedOtLHIHS"
   },
   "source": [
    "#### Note : You are limited to use 100 sorted data among the 1000 data in the given dataset, no more than 100 data is allowed for training !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TODO : select the simple or advanced dataset for training ##############\n",
    "\n",
    "dataset_used = \"sorted_dataset_advanced\" #sorted_dataset_advanced\n",
    "\n",
    "################# TODO ###################################################################\n",
    "\n",
    "if dataset_used == \"sorted_dataset_simple\":\n",
    "    train_dataset = sorted_dataset_simple.select(range(0,100))    ### You can also select from the middle, e.g. sorted_dataset_simple.select(range(50,150))\n",
    "else:\n",
    "    train_dataset = sorted_dataset_advanced.select(range(0,100))\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "train_dataset = standardize_sharegpt(train_dataset)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0n1YmtVUYxe"
   },
   "source": [
    "# Dataset Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'identity_41298',\n",
       " 'conversations': [{'content': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Format this following sentence correctly. ### Input: of course',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Of course.', 'role': 'assistant'}],\n",
       " 'score': 5.0,\n",
       " 'text': '<s>[INST] Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Format this following sentence correctly. ### Input: of course [/INST] Of course. </s>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfzTdMtvGE6w"
   },
   "source": [
    "And we see how the chat template transformed these conversations.\n",
    "\n",
    "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TPVyGbPUdJM"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f727679c8924a6c8744ab1b103d776a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 15,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,   ### Do not modify the seed for reproducibility\n",
    "}\n",
    "\n",
    "\n",
    "################# TODO #################################################################\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = training_config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps = training_config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps = training_config[\"warmup_steps\"],\n",
    "        num_train_epochs = training_config[\"num_train_epochs\"], # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = training_config[\"learning_rate\"],\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = training_config[\"optim\"],\n",
    "        weight_decay = training_config[\"weight_decay\"],\n",
    "        lr_scheduler_type = training_config[\"lr_scheduler_type\"],\n",
    "        seed = training_config[\"seed\"],\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeac4634ad504478aadb796524d5f5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"[INST]\",   # instruction 开始标记\n",
    "    response_part=\"[/INST]\",     # response 开始标记\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 15 | Total steps = 195\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,982,720 of 6,768,398,336 (0.44% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 19:56, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.792500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.250300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.784200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.774600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.945900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.336800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgII2E4bO97F"
   },
   "source": [
    "#### TODO : Curriculum Training  (Optional)\n",
    "start training the LLM with “easier” examples (e.g., shorter, clearer conversations) and progressively introduce more complex ones.\n",
    "\n",
    "The total data amount used to train should still not exceed 100 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_true_output(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the true assistant output from the decoded model output.\n",
    "    \n",
    "    Assumes LLaMA-3.1 style: <s>[INST] instruction [/INST] assistant_output </s>\n",
    "    \"\"\"\n",
    "    if \"[/INST]\" in text:\n",
    "        # assistant 输出部分在 [/INST] 之后\n",
    "        assistant_output = text.split(\"[/INST]\", 1)[1]\n",
    "    else:\n",
    "        # 如果没有 [/INST]，直接返回全部\n",
    "        assistant_output = text\n",
    "    \n",
    "    # 去掉可能存在的 EOS token 或首尾空白\n",
    "    assistant_output = assistant_output.replace(\"</s>\", \"\").strip()\n",
    "    \n",
    "    return assistant_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed for entry identity_8174\n",
      "Inference completed for entry identity_16675\n",
      "Inference completed for entry identity_51749\n",
      "Inference completed for entry identity_53196\n",
      "Inference completed for entry identity_65799\n",
      "Inference completed for entry identity_31686\n",
      "Inference completed for entry identity_25291\n",
      "Inference completed for entry identity_31699\n",
      "Inference completed for entry identity_30359\n",
      "Inference completed for entry identity_67085\n",
      "Inference completed for entry identity_60450\n",
      "Inference completed for entry identity_3070\n",
      "Inference completed for entry identity_36778\n",
      "Inference completed for entry identity_50478\n",
      "Inference completed for entry identity_20143\n",
      "Inference completed for entry identity_8300\n",
      "Inference completed for entry identity_45513\n",
      "Inference completed for entry identity_62606\n",
      "Inference completed for entry identity_38166\n",
      "Inference completed for entry identity_22233\n",
      "Inference completed for entry identity_54369\n",
      "Inference completed for entry identity_39141\n",
      "Inference completed for entry identity_15124\n",
      "Inference completed for entry identity_19488\n",
      "Inference completed for entry identity_66774\n",
      "Inference completed for entry identity_1646\n",
      "Inference completed for entry identity_938\n",
      "Inference completed for entry identity_13380\n",
      "Inference completed for entry identity_20131\n",
      "Inference completed for entry identity_59938\n",
      "Inference completed for entry identity_32956\n",
      "Inference completed for entry identity_32515\n",
      "Inference completed for entry identity_40418\n",
      "Inference completed for entry identity_53845\n",
      "Inference completed for entry identity_5949\n",
      "Inference completed for entry identity_8678\n",
      "Inference completed for entry identity_36333\n",
      "Inference completed for entry identity_16311\n",
      "Inference completed for entry identity_55093\n",
      "Inference completed for entry identity_49371\n",
      "Inference completed for entry identity_62747\n",
      "Inference completed for entry identity_1758\n",
      "Inference completed for entry identity_4555\n",
      "Inference completed for entry identity_53149\n",
      "Inference completed for entry identity_56125\n",
      "Inference completed for entry identity_47214\n",
      "Inference completed for entry identity_17337\n",
      "Inference completed for entry identity_21990\n",
      "Inference completed for entry identity_59710\n",
      "Inference completed for entry identity_58792\n",
      "Inference completed for entry identity_43526\n",
      "Inference completed for entry identity_54221\n",
      "Inference completed for entry identity_41687\n",
      "Inference completed for entry identity_49588\n",
      "Inference completed for entry identity_60653\n",
      "Inference completed for entry identity_25370\n",
      "Inference completed for entry identity_10581\n",
      "Inference completed for entry identity_24752\n",
      "Inference completed for entry identity_26314\n",
      "Inference completed for entry identity_69928\n",
      "Inference completed for entry identity_48622\n",
      "Inference completed for entry identity_69088\n",
      "Inference completed for entry identity_34275\n",
      "Inference completed for entry identity_47707\n",
      "Inference completed for entry identity_16427\n",
      "Inference completed for entry identity_3696\n",
      "Inference completed for entry identity_44369\n",
      "Inference completed for entry identity_67251\n",
      "Inference completed for entry identity_43324\n",
      "Inference completed for entry identity_14506\n",
      "Inference completed for entry identity_55612\n",
      "Inference completed for entry identity_38504\n",
      "Inference completed for entry identity_212\n",
      "Inference completed for entry identity_41110\n",
      "Inference completed for entry identity_11932\n",
      "Inference completed for entry identity_39966\n",
      "Inference completed for entry identity_60276\n",
      "Inference completed for entry identity_68912\n",
      "Inference completed for entry identity_52637\n",
      "Inference completed for entry identity_11474\n",
      "Inference completed for entry identity_37568\n",
      "Inference completed for entry identity_4687\n",
      "Inference completed for entry identity_44577\n",
      "Inference completed for entry identity_55142\n",
      "Inference completed for entry identity_42855\n",
      "Inference completed for entry identity_44485\n",
      "Inference completed for entry identity_69256\n",
      "Inference completed for entry identity_68094\n",
      "Inference completed for entry identity_11188\n",
      "Inference completed for entry identity_43756\n",
      "Inference completed for entry identity_30073\n",
      "Inference completed for entry identity_18350\n",
      "Inference completed for entry identity_23194\n",
      "Inference completed for entry identity_12048\n",
      "Inference completed for entry identity_3455\n",
      "Inference completed for entry identity_36517\n",
      "Inference completed for entry identity_44691\n",
      "Inference completed for entry identity_9677\n",
      "Inference completed for entry identity_2281\n",
      "Inference completed for entry identity_11769\n",
      "Inference completed for entry identity_41764\n",
      "Inference completed for entry identity_13733\n",
      "Inference completed for entry identity_55557\n",
      "Inference completed for entry identity_63707\n",
      "Inference completed for entry identity_22872\n",
      "Inference completed for entry identity_32258\n",
      "Inference completed for entry identity_18632\n",
      "Inference completed for entry identity_29514\n",
      "Inference completed for entry identity_6860\n",
      "Inference completed for entry identity_3319\n",
      "Inference completed for entry identity_2140\n",
      "Inference completed for entry identity_39471\n",
      "Inference completed for entry identity_10759\n",
      "Inference completed for entry identity_5721\n",
      "Inference completed for entry identity_50510\n",
      "Inference completed for entry identity_28099\n",
      "Inference completed for entry identity_18060\n",
      "Inference completed for entry identity_12298\n",
      "Inference completed for entry identity_49746\n",
      "Inference completed for entry identity_63808\n",
      "Inference completed for entry identity_40739\n",
      "Inference completed for entry identity_3363\n",
      "Inference completed for entry identity_65554\n",
      "Inference completed for entry identity_54315\n",
      "Inference completed for entry identity_393\n",
      "Inference completed for entry identity_69938\n",
      "Inference completed for entry identity_42000\n",
      "Inference completed for entry identity_27410\n",
      "Inference completed for entry identity_29274\n",
      "Inference completed for entry identity_704\n",
      "Inference completed for entry identity_12567\n",
      "Inference completed for entry identity_23393\n",
      "Inference completed for entry identity_33134\n",
      "Inference completed for entry identity_37372\n",
      "Inference completed for entry identity_32509\n",
      "Inference completed for entry identity_8768\n",
      "Inference completed for entry identity_68493\n",
      "Inference completed for entry identity_8640\n",
      "Inference completed for entry identity_23939\n",
      "Inference completed for entry identity_21111\n",
      "Inference completed for entry identity_50901\n",
      "Inference completed for entry identity_23500\n",
      "Inference completed for entry identity_36745\n",
      "Inference completed for entry identity_26063\n",
      "Inference completed for entry identity_35418\n",
      "Inference completed for entry identity_68496\n",
      "Inference completed for entry identity_57544\n",
      "Inference completed for entry identity_25443\n",
      "Inference completed for entry identity_47264\n",
      "Inference completed for entry identity_631\n",
      "Inference completed for all entries in the test set.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Load the test set JSON file (without GPT responses)\n",
    "with open(\"ML_Spring2025_HW5/test_set_evol_instruct_150.json\", \"r\") as infile:\n",
    "    test_data = json.load(infile)\n",
    "\n",
    "# Dictionary to store inference results\n",
    "inference_results = {}\n",
    "\n",
    "# Loop over each data entry in the test set\n",
    "for index,entry in enumerate(test_data):\n",
    "    entry_id = entry.get(\"id\", \"unknown_id\")\n",
    "\n",
    "    # Build the messages list from the human conversation entries\n",
    "    # (Test set is expected to have only \"human\" messages)\n",
    "    messages = []\n",
    "    for conv in entry.get(\"conversations\", []):\n",
    "        if conv.get(\"from\") == \"human\":\n",
    "            messages.append({\"role\": \"user\", \"content\": conv.get(\"value\", \"\")})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": conv.get(\"value\", \"\")})\n",
    "\n",
    "    # Create inputs using the chat template (required for generation)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Must add for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "\n",
    "################# TODO : Tweak Decoding Parameters here.  #####################\n",
    "\n",
    "\n",
    "    # Generate model outputs\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p = 0.9,\n",
    "        top_k = 30,\n",
    "    )\n",
    "\n",
    "\n",
    "################# TODO  ##########################################################\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    # Parse each output to extract the true assistant response\n",
    "    parsed_outputs = [parse_true_output(output) for output in decoded_outputs]\n",
    "\n",
    "    # Store the result for the current entry\n",
    "    inference_results[entry_id] = {\n",
    "        \"input\": messages,\n",
    "        \"output\": parsed_outputs\n",
    "    }\n",
    "\n",
    "    print(f\"Inference completed for entry {entry_id}\")\n",
    "\n",
    "\n",
    "#Write the inference results to the prediction JSON file\n",
    "with open(f\"pred.json\", \"w\") as outfile:\n",
    "    json.dump(inference_results, outfile, indent=4)\n",
    "with open(f\"training_config.json\", \"w\") as outfile:\n",
    "    json.dump(training_config, outfile, indent=4)\n",
    "\n",
    "print(\"Inference completed for all entries in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSz-CYXjIXVX"
   },
   "source": [
    "## Saving, loading finetuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLDgDmo5IqBc"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPYojUG-Ivtn"
   },
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.7: Fast Llama patching. Transformers: 4.55.4. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.928 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_model\", # The folder path containing of the folder that contains adapter_model.safetensors, adapter_config.json and README.md\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
